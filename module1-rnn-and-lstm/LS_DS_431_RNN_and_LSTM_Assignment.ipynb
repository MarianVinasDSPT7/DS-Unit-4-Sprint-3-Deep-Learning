{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import  Dense, LSTM\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "\n",
    "r = requests.get(url)\n",
    "r.encoding = r.apparent_encoding\n",
    "data = r.text\n",
    "data = data.split('\\r\\n')\n",
    "toc = [l.strip() for l in data[44:130:2]]\n",
    "# Skip the Table of Contents\n",
    "data = data[135:]\n",
    "\n",
    "# Fixing Titles\n",
    "toc[9] = 'THE LIFE OF KING HENRY V'\n",
    "toc[18] = 'MACBETH'\n",
    "toc[24] = 'OTHELLO, THE MOOR OF VENICE'\n",
    "toc[34] = 'TWELFTH NIGHT: OR, WHAT YOU WILL'\n",
    "\n",
    "locations = {id_:{'title':title, 'start':-99} for id_,title in enumerate(toc)}\n",
    "\n",
    "# Start \n",
    "for e,i in enumerate(data):\n",
    "    for t,title in enumerate(toc):\n",
    "        if title in i:\n",
    "            locations[t].update({'start':e})\n",
    "            \n",
    "\n",
    "df_toc = pd.DataFrame.from_dict(locations, orient='index')\n",
    "df_toc['end'] = df_toc['start'].shift(-1).apply(lambda x: x-1)\n",
    "df_toc.loc[42, 'end'] = len(data)\n",
    "df_toc['end'] = df_toc['end'].astype('int')\n",
    "\n",
    "df_toc['text'] = df_toc.apply(lambda x: '\\r\\n'.join(data[ x['start'] : int(x['end']) ]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE TRAGEDY OF ANTONY AND CLEOPATRA</td>\n",
       "      <td>-99</td>\n",
       "      <td>14379</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AS YOU LIKE IT</td>\n",
       "      <td>14380</td>\n",
       "      <td>17171</td>\n",
       "      <td>AS YOU LIKE IT\\r\\n\\r\\n\\r\\nDRAMATIS PERSONAE.\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THE COMEDY OF ERRORS</td>\n",
       "      <td>17172</td>\n",
       "      <td>20372</td>\n",
       "      <td>THE COMEDY OF ERRORS\\r\\n\\r\\n\\r\\n\\r\\nContents\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE TRAGEDY OF CORIOLANUS</td>\n",
       "      <td>20373</td>\n",
       "      <td>30346</td>\n",
       "      <td>THE TRAGEDY OF CORIOLANUS\\r\\n\\r\\nDramatis Pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CYMBELINE</td>\n",
       "      <td>30347</td>\n",
       "      <td>30364</td>\n",
       "      <td>CYMBELINE.\\r\\nLaud we the gods;\\r\\nAnd let our...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  start    end  \\\n",
       "0  THE TRAGEDY OF ANTONY AND CLEOPATRA    -99  14379   \n",
       "1                       AS YOU LIKE IT  14380  17171   \n",
       "2                 THE COMEDY OF ERRORS  17172  20372   \n",
       "3            THE TRAGEDY OF CORIOLANUS  20373  30346   \n",
       "4                            CYMBELINE  30347  30364   \n",
       "\n",
       "                                                text  \n",
       "0                                                     \n",
       "1  AS YOU LIKE IT\\r\\n\\r\\n\\r\\nDRAMATIS PERSONAE.\\r...  \n",
       "2  THE COMEDY OF ERRORS\\r\\n\\r\\n\\r\\n\\r\\nContents\\r...  \n",
       "3  THE TRAGEDY OF CORIOLANUS\\r\\n\\r\\nDramatis Pers...  \n",
       "4  CYMBELINE.\\r\\nLaud we the gods;\\r\\nAnd let our...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shakespeare Data Parsed by Play\n",
    "df_toc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15205845, 3041161, 3041161)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gather all text to see all possible characters, for training/splitting\n",
    "\n",
    "data = df_toc['text'].values\n",
    "text = \" \".join(data)\n",
    "\n",
    "# unique caracters\n",
    "chars = list(set(text))\n",
    "\n",
    "# lookup tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "# create the sequence data\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = []\n",
    "next_char = []\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "\n",
    "len(encoded), len(sequences), len(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "95037/95037 [==============================] - ETA: 0s - loss: 1.7303\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \" you are not void of pity.\n",
      "    I sold n\"\n",
      " you are not void of pity.\n",
      "    I sold not your goptain’d was revenged,\n",
      "    May, been hither favour, floom!—Most tell the world;\n",
      "Than he see Agling, Kinuserenring; let Eggue?\n",
      "  SHARLOCL. A firtence on his cellence of Sempor will\n",
      "    And then, but in this bears him unda, NobbyOr very.\n",
      "    Steet\n",
      "With a brother; when it comes up of dames. And a repience of looks,\n",
      "    Of this in remember father he come.\n",
      "\n",
      "FATSEMONN.\n",
      "Tould-banked, a\n",
      "95037/95037 [==============================] - 3615s 38ms/step - loss: 1.7303\n",
      "Epoch 2/10\n",
      "95037/95037 [==============================] - ETA: 0s - loss: 1.4873\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"And that thou art so, there I throw my g\"\n",
      "And that thou art so, there I throw my grief,\n",
      "That so not him oft seem as kingly\n",
      "So (at generomal expronish of himself.\n",
      "\n",
      "ROCLOR.\n",
      "O, Ihto: how I am her; in my will I did will\n",
      "Which swear the matter long and woof my lork,\n",
      "My life by our put-thou, hunbly love,\n",
      "The imbrition castire, when they may have not night is please of davising readed,\n",
      "To an and to mer to do his not.\n",
      "But thou tours dies, is out of sheid, charred and defen ev\n",
      "95037/95037 [==============================] - 3072s 32ms/step - loss: 1.4873\n",
      "Epoch 3/10\n",
      "95036/95037 [============================>.] - ETA: 0s - loss: 1.4298\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"ill cry: where, and how, & wherfore? you\"\n",
      "ill cry: where, and how, & wherfore? your grace I not\n",
      "With him re both somethy.\n",
      "\n",
      "‘“Or noble had marketies Luconts, advise wink; confeshimous in the myshines,\n",
      "Sir check thou art tenours kill and sost parsen, but did let man depies\n",
      "    Brother, many wer, through been-sick sout.\n",
      "  YORK. What ramb that I come, law remory:\n",
      "  Thought letter friend scaps on him; but my lord\n",
      "    On faurr's groundy, Augrow, my lords, thy Apperous that ha\n",
      "95037/95037 [==============================] - 3216s 34ms/step - loss: 1.4298\n",
      "Epoch 4/10\n",
      "95036/95037 [============================>.] - ETA: 0s - loss: 1.4002\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"row!\n",
      "         O, what may man within hi\"\n",
      "row!\n",
      "         O, what may man within him opeent,\n",
      "    That his strong you.\n",
      "  SHALLSEY. To roys but sweet a boed me-sum, in thine\n",
      "To for the cannon Roman, Pray in they?\n",
      "    Arize and the truth in my guests, sure?\n",
      "\n",
      "MERCUTIO.\n",
      "Somers’ tucket, my loft, let me can blood, appow'd.\n",
      "  most me again allimines of the lof behtow\n",
      "    So not friend I small and igain'd in furiflive;\n",
      "I be truither or sames of a way if I stame,\n",
      "Duke doth thun\n",
      "95037/95037 [==============================] - 3193s 34ms/step - loss: 1.4002\n",
      "Epoch 5/10\n",
      "95036/95037 [============================>.] - ETA: 0s - loss: 1.3816\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"ry in the Tower,\n",
      "    And Edward, my poo\"\n",
      "ry in the Tower,\n",
      "    And Edward, my poor gaust arrive at your broken o'll\n",
      "    Till when it will to call it honour- I know,\n",
      "    To trust the surce- a time by her comes;\n",
      "    Or I will say their Rome of the Natchator.\n",
      "\n",
      "\n",
      "      ERIS.\n",
      " y, light, when I we not your faults shall from him.\n",
      "    No grod, which once exeut on, will you but\n",
      "our court, exe curstly lassesmed for turn\n",
      "Where' the myself, nor visitable hous,\n",
      "      Even fit so.\n",
      "95037/95037 [==============================] - 3185s 34ms/step - loss: 1.3816\n",
      "Epoch 6/10\n",
      "95036/95037 [============================>.] - ETA: 0s - loss: 1.3691\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \" live,\n",
      "    And buried once, why not upo\"\n",
      " live,\n",
      "    And buried once, why not upon staght a drick\n",
      "      Othellow'd air shake a cut so, hark, I’ll he\n",
      "And mother that charge it, and not no ded.\n",
      "\n",
      "THIRD LORD.\n",
      "Good fond, good lives is true nic; sir,\n",
      "Proves you. I haill'd mostent. I know it,\n",
      "    Dastaking oven to them, corongaations with me!\n",
      "\n",
      "HERMIA.\n",
      "And by what off prite, then-I to there nay\n",
      "with wick'd the Enfedd, devils like your word\n",
      "      Do the will flyer honourabl\n",
      "95037/95037 [==============================] - 3190s 34ms/step - loss: 1.3691\n",
      "Epoch 7/10\n",
      "95037/95037 [==============================] - ETA: 0s - loss: 1.3592\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"   Re-enter SALISBURY\n",
      "\n",
      "  SALISBURY. [T\"\n",
      "   Re-enter SALISBURY\n",
      "\n",
      "  SALISBURY. [To FALITON. O bad are then likely gear; and I fend Bad,\n",
      "  But of them wake it in fly paid by deed,\n",
      "    Must spent must follow’d, should co"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ast at friend\n",
      "Back to plead host that if again.\n",
      "\n",
      " [_Exit Betray shall I dare blood York un conveint\n",
      "    horve; not him?\n",
      "  PETER. That simper phill, nor sire call me. Traitor is done for him.\n",
      "\n",
      "KIIG Henry I attending the Fires\n",
      "\n",
      "  EDWARD. He protee one wo\n",
      "95037/95037 [==============================] - 3191s 34ms/step - loss: 1.3592\n",
      "Epoch 8/10\n",
      "95036/95037 [============================>.] - ETA: 0s - loss: 1.3522\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"cks, foul stockings, greasy\n",
      "    napkins\"\n",
      "cks, foul stockings, greasy\n",
      "    napkins.\n",
      "\n",
      " [_Exeunt._]\n",
      "\n",
      "\n",
      "\n",
      "HOLTEUS\n",
      "\n",
      "  bright; I leasn me treass of once lion now?\n",
      "  SPEED. Ay, thou art a love.\n",
      "  COBILLERY. woman bawd\n",
      "For part of the posseman people, concerfuctos\n",
      "Die speak of one wert makes weak begging.\n",
      "  COTIO. Ay, sit in serve plague in his a more?\n",
      "  TIMON. Good a dieds with the tomb, to the troadion\n",
      "      ’twas I speak thee is afontune from owcate;\n",
      "    Natuenes diss\n",
      "95037/95037 [==============================] - 3205s 34ms/step - loss: 1.3522\n",
      "Epoch 9/10\n",
      "95035/95037 [============================>.] - ETA: 0s - loss: 1.3615\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"h all her best endowments, all those beu\"\n",
      "h all her best endowments, all those beutth.\n",
      "\n",
      "PETER.\n",
      "Ay, no?\n",
      "\n",
      "THESEUS.\n",
      "If it is.\n",
      "\n",
      "PANDARUS.\n",
      "O! he is, to fools none more. Good chemila.\n",
      "\n",
      "CAMILLO.\n",
      "_Retold thee worth, enall when\n",
      "you’d that not device well had no youth.\n",
      "\n",
      "PAULINA.\n",
      "Welcome.\n",
      "\n",
      "PALAMON.\n",
      "Yet I say; not King, and your life, the pilc’ fonger.\n",
      "\n",
      "FALSTALCES.\n",
      "[_Aside._] _How is not the Most weephery's finmer.\n",
      "  KING and Yonder comples as gross\n",
      "Whose weaple thi\n",
      "95037/95037 [==============================] - 3196s 34ms/step - loss: 1.3615\n",
      "Epoch 10/10\n",
      "95035/95037 [============================>.] - ETA: 0s - loss: 1.3450\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"s from frieze,\n",
      "It plucks out brains and\"\n",
      "s from frieze,\n",
      "It plucks out brains and my love, I have embly, or met.\n",
      "And you darely in safely by the quickles in good so\n",
      "Are you shape! It is late, with my march\n",
      "tongue, against us will be with melting it.\n",
      "\n",
      "‘With my own dishonour's youth as the mere.\n",
      "     when tomidate of may I can let me for having letter?\n",
      "    Weall'd with mingific of CLAIR' SCY\n",
      "Half works! Cell understive not Ay, but yourself,\n",
      "    And have respect another \n",
      "95037/95037 [==============================] - 3201s 34ms/step - loss: 1.3450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd9b8f0d828>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create x & y\n",
    "\n",
    "import random\n",
    "import sys\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1\n",
    "\n",
    "# build model - single LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    # Random prompt\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        # Predict the next step (character)\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "# fit the model\n",
    "model.fit(x, y,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUTION FROM BRUNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution from Bruno\n",
    "sonnets = open(\"sonnets.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.23.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
